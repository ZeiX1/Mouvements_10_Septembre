{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6920bcaf-ee52-4417-be82-b65fd07c15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e955d39-9709-42a9-8d15-64102a30326c",
   "metadata": {},
   "source": [
    "## Fonctions importantes pour le nettoyage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a514ab-120d-4a5c-9883-8ceaa477f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", text)  # enlever URLs\n",
    "    text = re.sub(r\"[^a-zA-Zàâçéèêëîïôûùüÿñæœ\\s]\", \" \", text)  # enlever ponctuation/chiffres\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a99a1-761f-4d97-bb0d-35688a83b310",
   "metadata": {},
   "source": [
    "### Fonction qui suppriment les stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2703cae-f539-49d2-93ac-2d0c8c919fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "x = [\n",
    "    \"abord\", \"afin\", \"ah\", \"aie\", \"ainsi\", \"allo\", \"allô\", \"alors\", \"après\", \"assez\", \"attendu\", \"aucun\", \"aucune\", \"aucuns\",\n",
    "    \"aujourd\", \"aujourd'hui\", \"auquel\", \"aura\", \"auront\", \"aussi\", \"autre\", \"autres\", \"aux\", \"auxquelles\", \"auxquels\", \"avaient\",\n",
    "    \"avais\", \"avait\", \"avant\", \"avec\", \"avoir\", \"avons\", \"ayant\", \"bah\", \"bas\", \"beaucoup\", \"bien\", \"c\", \"c'est\", \"ça\", \"car\",\n",
    "    \"ce\", \"ceci\", \"cela\", \"ces\", \"cet\", \"cette\", \"ceux\", \"ceux-ci\", \"ceux-là\", \"chaque\", \"chez\", \"chose\", \"ci\", \"comme\", \"comment\",\n",
    "    \"contre\", \"couic\", \"d\", \"dans\", \"de\", \"debout\", \"dedans\", \"dehors\", \"def\", \"depuis\", \"derrière\", \"des\", \"désormais\", \"desquelles\",\n",
    "    \"desquels\", \"dessous\", \"dessus\", \"deux\", \"deuxième\", \"devant\", \"devers\", \"devra\", \"devront\", \"difficile\", \"dire\", \"dit\", \"dite\",\n",
    "    \"dits\", \"divers\", \"diverse\", \"diverses\", \"doit\", \"doivent\", \"donc\", \"dont\", \"dos\", \"d'autres\", \"du\", \"duquel\", \"duquelles\",\n",
    "    \"duquels\", \"durant\", \"e\", \"eh\", \"elle\", \"elle-même\", \"elles\", \"elles-mêmes\", \"en\", \"encore\", \"entre\", \"envers\", \"environ\",\n",
    "    \"es\", \"est\", \"est-ce\", \"et\", \"etant\", \"etc\", \"étaient\", \"étais\", \"était\", \"étant\", \"être\", \"eu\", \"eue\", \"eues\", \"eux\", \"eux-mêmes\",\n",
    "    \"excepté\", \"existe\", \"existe-t-il\", \"f\", \"faut\", \"faut-il\", \"femme\", \"femmes\", \"feront\", \"fête\", \"fêtes\", \"fin\", \"fini\", \"finir\", \"finit\",\n",
    "    \"finitive\", \"finitives\", \"fois\", \"font\", \"forcément\", \"fort\", \"forte\", \"fortes\", \"forts\", \"fou\", \"fouiller\", \"fouillé\", \"fouillée\",\n",
    "    \"fouillés\", \"fous\", \"fut\", \"fut-ce\", \"fut-il\", \"fut-elle\", \"fut-on\", \"fut-ils\",\"si\",\"ici\",\"où\",\"là\",\"très\",\"parce\",\"va\",\"ca\",\"oui\",\"non\",\"tout\",\"toute\"\n",
    "] + list(\"abcdefghijklmnopqrstuvwxyz\") \n",
    "stop_words = set(stopwords.words('french'))\n",
    "stop_words = stop_words | set(x)\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    words_clean = [w for w in words if w.lower() not in stop_words]\n",
    "    return \" \".join(words_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4592d2e4-1474-408c-80f6-f9d8c869963a",
   "metadata": {},
   "source": [
    " ### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2cd37-bb3a-4186-b6b5-5d464242ee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# chemins vers dossiers, à modifier en fonction de l'environnement de travail\n",
    "folder1 = \"scraped_messages_1509/\"\n",
    "folder2 = \"scraped_messages_1609/\"\n",
    "output_folder = \"merged_messages1\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# on récupère la liste des fichiers CSV du premier dossier\n",
    "for filename in os.listdir(folder1):\n",
    "    if filename.endswith(\".csv\") and filename in os.listdir(folder2):\n",
    "        file1 = os.path.join(folder1, filename)\n",
    "        file2 = os.path.join(folder2, filename)\n",
    "\n",
    "        df1 = pd.read_csv(file1)\n",
    "        df2 = pd.read_csv(file2)\n",
    "\n",
    "        merged = pd.concat([df1, df2], ignore_index=True)\n",
    "        # suppression des doublons sur message_id & date\n",
    "        merged = merged.drop_duplicates(subset=[\"message_id\", \"date\"])\n",
    "        merged = merged.sort_values(by = \"date\")\n",
    "\n",
    "        output_file = os.path.join(output_folder, filename)\n",
    "        merged.to_csv(output_file, index=False)\n",
    "        print(f\"Fichier fusionné sauvegardé : {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a53c603-40f1-42a4-a197-7f902c3b1a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dossier contenant les CSV fusionnés\n",
    "input_folder = \"merged_messages1/\"\n",
    "output_file = \"all_merged_messages.csv\"\n",
    "\n",
    "# liste des fichiers CSV\n",
    "csv_files = [f for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# concaténation de tous les CSV\n",
    "dfs = []\n",
    "for file in csv_files:\n",
    "    path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(path)\n",
    "    dfs.append(df)\n",
    "\n",
    "# fusion finale\n",
    "final_merged = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# suppression des doublons (au cas où)\n",
    "final_merged = final_merged.drop_duplicates(subset=[\"message_id\", \"date\"])\n",
    "final_merged = final_merged.sort_values(by=\"date\")\n",
    "\n",
    "# sauvegarde\n",
    "final_merged.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"✅ Tous les fichiers ont été fusionnés dans : {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a2dbf2-81c8-4e09-8aa7-06435ea9e82c",
   "metadata": {},
   "source": [
    " #### Applications des fonctions créées tout à l'heure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e7aad-2d17-4a59-b5c3-946cead1eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"clean\"] = data[\"text\"].apply(clean_text)\n",
    "data[\"clean_stopwords\"] = data[\"clean\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65baf0e5-636b-45ec-a375-7ee8728940de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_words = \" \".join(data[\"clean_stopwords\"]).split()\n",
    "counter = Counter(all_words)\n",
    "\n",
    "# 20 mots les plus fréquents\n",
    "print(counter.most_common(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7927aa84-f6cc-4c03-a90b-4eb2eb33ff4a",
   "metadata": {},
   "source": [
    "#### On gère les lignes qui n'ont que très peu de mots (souvent des non-sens comme \"jzzjz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af6def-5f0c-493e-8db0-961577e55629",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"clean_stopword\"].str.split().str.len() >= 3].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7612b0f8-a8ca-499e-8238-305218c55c7c",
   "metadata": {},
   "source": [
    "## Début de l'analyse du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814b085f-4870-48d8-85c6-896f6beb2dff",
   "metadata": {},
   "source": [
    "## Ici on combine TfIDF à LDA (Latent Dirichlet Allocation) \n",
    "TfIDF est une méthode de vectorisation, le texte étant par définition une suite de caractères, il faut trouver un moyen de le numériser tout en gardant le contexte et la fréquence d'apparitions des termes. TfIDF prend un mot dans un Document( pour nous un message = un Document) et retourne sa fréquence dans le corpus suivant la fonction : $$ Tf_{ij} = \\frac{f_{t,d}}{\\sum_{t'\\in d}f_{t',d}}$$ où $f_{t,d}$ correspond à la fréquence d'apparition du mot $t$ dans le document $d$. La fréquence inverse est donnée par $$Idf_i = log(\\frac{|D|}{|d, t_i \\in d|}), \\text{avec |D| le nombre total de messages}$$. Au final TfIDF est donnée par $$ TfIDF_{ij} = Tf_{ij}\\cdot Idf_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1610e298-5f94-457d-acf0-8b0d20249fc8",
   "metadata": {},
   "source": [
    "#### LDA\n",
    "L'algorithme LDA (Latent Dirichlet Allocation) permet d'analyser un corpus de document et permet de faire de la classification rapidement. L'algorithme divise en \"topics\" le document et assigne à chaque mot un topic. (Voir le papier original pour l'implémentation, je n'ai pas eu le temps de regarder !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39796f5-06a8-4018-b584-2f9979026e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "texts = data['clean_stopword'].dropna().astype(str) #remove NAs\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.9, min_df=5) #les paramètres permettent de supprimer des termes trop/pas assez fréquents de l'analyse\n",
    "X = vectorizer.fit_transform(texts)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# =========================\n",
    "#  Modèle LDA\n",
    "# =========================\n",
    "n_topics = 5 \n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# =========================\n",
    "# =========================\n",
    "no_top_words = 10\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"\\nTopic {topic_idx}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "# =========================\n",
    "# =========================\n",
    "topic_values = lda.transform(X)\n",
    "data['dominant_topic'] = topic_values.argmax(axis=1)\n",
    "\n",
    "# =========================\n",
    "#  WordCloud par topic\n",
    "# =========================\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    topic_words = {feature_names[i]: topic[i] for i in topic.argsort()[:-50 - 1:-1]}  # top 50 mots\n",
    "    wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(topic_words)\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"WordCloud Topic {topic_idx}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d303338-1717-4770-870b-f3dccad666c2",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "C'est une autre méthode de vectorisation, qui garde le contexte et le sens des mots, mais requiert bien plus de calculs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b087c-5c8c-407b-8091-0e4d0c5a6be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "# -------------------------------\n",
    "# MiniLM est rapide et léger (384 dimensions)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cffdb2-b4a6-40e9-9386-6d28a0d1cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4. Calcul des embeddings par batch\n",
    "# -------------------------------\n",
    "batch_size = 5000\n",
    "embeddings = []\n",
    "\n",
    "for start_idx in tqdm(range(0, len(data), batch_size)):\n",
    "    end_idx = min(start_idx + batch_size, len(data))\n",
    "    batch_texts = data['clean'].iloc[start_idx:end_idx].tolist()\n",
    "    batch_embeddings = model.encode(batch_texts, show_progress_bar=False)\n",
    "    embeddings.append(batch_embeddings)\n",
    "\n",
    "# Fusionner tous les batches en un seul array\n",
    "embeddings = np.vstack(embeddings)\n",
    "print(\"Shape des embeddings :\", embeddings.shape)  # (150000, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945f5ff1-9019-4321-8ed9-a0bee0e60e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = pd.concat([data.reset_index(drop=True), pd.DataFrame(embeddings)], axis=1)\n",
    "table = pa.Table.from_pandas(emb_df)\n",
    "pq.write_table(table, \"messages_with_embeddings.parquet\")\n",
    "\n",
    "print(\"Embeddings calculés et stockés avec succès !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2511889-6e18-44bb-89d4-8fd6ad8121e4",
   "metadata": {},
   "source": [
    "#### Lire le fichier parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d7268-7711-4f1e-a185-22c026ecbf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_emb = pd.read_parquet(\"messages_with_embeddings.parquet\",engine = \"fastparquet\")\n",
    "print(df_emb.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91efcfbf-6a47-4078-998d-b35c0985a527",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = [  #Liste qui contient chaque colonne qui n'est pas une des dimensions du embedding (à faire avant avec pd.col)\n",
    "    \"chat\",\n",
    "    \"chat_name\",\n",
    "    \"message_id\",\n",
    "    \"date\",\n",
    "    \"sender_id\",\n",
    "    \"text\",\n",
    "    \"reply_to\",\n",
    "    \"clean\",\n",
    "    \"clean_stopword\",\n",
    "    \"nb_mots_champ\",\n",
    "    \"somme_mots\",\n",
    "    \"nb_champ_relatif\",\n",
    "    \"clean_no_stop\"\n",
    "]\n",
    "  # adapter selon tes colonnes\n",
    "embedding_columns = df_emb.columns.difference(columns_list)\n",
    "\n",
    "X = df_emb[embedding_columns].values\n",
    "\n",
    "# Normalisation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "## clustering par K-means\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 10  # tu peux ajuster\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df_emb['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "print(df_emb[['text', 'cluster']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f773671-058d-4979-94a3-28bba9853bec",
   "metadata": {},
   "source": [
    "### Representation graphique du clustering, problème étant que l'on joue avec des données de 380 dimensions.\n",
    "On utilise le package umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f52e4-34bc-4a86-b5d3-fedd4a4070b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "reducer = umap.UMAP(n_components=2, random_state=42,unique = True)\n",
    "X_umap = reducer.fit_transform(X_scaled)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "X_umap = reducer.fit_transform(X_scaled)\n",
    "\n",
    "df_emb['x'] = X_umap[:,0]\n",
    "df_emb['y'] = X_umap[:,1]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(data=df_emb, x='x', y='y', hue='cluster', palette='tab10', s=50, alpha=0.8)\n",
    "plt.title(\"Clustering des messages\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
